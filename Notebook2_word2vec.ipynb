{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2f387a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "583976a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12e206",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "In any NLP task with neural networks involved, we need a numerical representation of our input (which are mainly words). A naive solution would be to use a huge one-hot vector with the same size as our vocabulary, each element representing one word. But this sparse representation is a poor usage of a huge multidimentional space as it does not contain any usefull information about the meaning and semantics of a word. This is where word embedding comes in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97189ebf",
   "metadata": {},
   "source": [
    "## 1. What is a word embedding?\n",
    "Embeddings are another way of representing vocabulary in a lower dimentional (compared to one-hot representation) continuous space. The goal is to have similar vectors for the words with similar meanings (so the elements of the vector actually carry some information about the meaning of the words). The question is, how are we going to achieve such representations? The idea is simple but elegant: The words appearing in the same context are likely to have similar meanings.\n",
    "\n",
    "So how can we use this idea to learn word vectors?\n",
    "\n",
    "## 2. How to train?\n",
    "We are going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer and use this hidden layer as our word representation vector.\n",
    "\n",
    "So lets talk about this \"fake\" task. We’re going to train the neural network to do the following: given a specific word (the input word), the network is going to tell us the probability for every word in our vocabulary of being near to this given word (be one of its context words). So the network is going to look somthing like this (considering that our vocabulary size is 10000):\n",
    "\n",
    "<img src=\"skip_gram_net_arch.png\">\n",
    "\n",
    "By training the network on this task, the words which appear in similar contexts are forced to have similar values in the hidden layer since they are going to give similar outputs, so we can use this hidden layer values as our word representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2e1ee",
   "metadata": {},
   "source": [
    "## 3. Softmax\n",
    "\n",
    "Softmax is a very handy tool when it comes to probability distribution prediction problems, but it has its downsides when the number of the nodes grows too large. Let's look at softmax activation in our output layer:\n",
    "\n",
    "<img src=\"softmax.PNG\">\n",
    "\n",
    "As you can see, every single output is dependent on the other outputs, so in order to compute the derivative with respect to any weight, all the other weights play a role! For a 10000 output size this results in milions of mathematical operations for a single weight update, which is not practical at all! This is why the original authors of the paper implemented \"Negative sampling\". By selecting only a fragment of the weights to be updated, they were able to get a huge speed gain.\n",
    "\n",
    "## 4. Word2vec in Gensim\n",
    "\n",
    "There is a vergy good library called gensim for using word2vec in python. You can train your own word vectors on your own corpora or use available pretrained models. Let's train a word2vec model on the Stackoverflow data set and see what comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaff4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    data['tags'] = data['tags'].apply(literal_eval)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9175ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('data/train.tsv')\n",
    "validation = read_data('data/validation.tsv')\n",
    "test = pd.read_csv('data/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8dcd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_strings = train['title'].tolist() + validation['title'].tolist() + test['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a22d13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97482c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()# lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE,' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(BAD_SYMBOLS_RE,'',text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = list(filter(lambda x: x not in STOPWORDS,  text.split()))# delete stopwords from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d65b4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in training_strings:\n",
    "            line = text_prepare(line)\n",
    "            # assume there's one document per line, tokens separated by whitespace. Don't forget to use your preprocessing function from the last notebook\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50bc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688f7ee6",
   "metadata": {},
   "source": [
    "The window parameter determines the size of the window (surrounding words in either direction) that the embeddings are learned on. If the window size is small we tend to capture more synonyms (e.g. window=3). If we make it larger, we tend to capture more semantically related domains words (e.g. window=7). \n",
    "\n",
    "By changing the different vector_size parameters we can adjust the dimensionality of our embeddings and capture more finegrained similarities.\n",
    "\n",
    "Play around with the different settings to see this in action. You can check the nearest words in the vector space for a given words with the 'model.wv.most_similar(<word>)' function.\n",
    "    \n",
    "Since our dataset is not very large, less frequent words will have less good embeddings than the most frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3844544",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=sentences, vector_size=300, window=2, min_count=3, workers=4)\n",
    "#model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "103220fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take random word from keyed vectors to see if tokenisation worked well\n",
    "#import random\n",
    "#random.choice(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6071e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to: python \n",
      " [('r', 0.649116575717926), ('pythons', 0.6010829210281372), ('java', 0.5737391114234924), ('numpy', 0.5535851120948792), ('ruby', 0.547137975692749), ('4d', 0.5386043190956116), ('c', 0.5193857550621033), ('scipy', 0.5170186161994934), ('bash', 0.5102644562721252), ('rscript', 0.5091903209686279)] \n",
      "\n",
      "\n",
      "Most similar words to: java \n",
      " [('c#', 0.6929925084114075), ('vbnet', 0.5779470801353455), ('python', 0.5737391114234924), ('c', 0.5429466962814331), ('writing', 0.527299165725708), ('c++', 0.522455096244812), ('listcontains', 0.5204864144325256), ('4d', 0.5177037119865417), ('messy', 0.5111239552497864), ('receives', 0.5105389356613159)] \n",
      "\n",
      "\n",
      "Most similar words to: javascript \n",
      " [('js', 0.7135972380638123), ('jqueryget', 0.6935321092605591), ('jquery', 0.6753265857696533), ('ajax', 0.6574794054031372), ('onclick', 0.6528342366218567), ('tags', 0.6347712874412537), ('ready', 0.6227518320083618), ('php', 0.6185012459754944), ('jstl', 0.6140502691268921), ('jquerys', 0.611627459526062)] \n",
      "\n",
      "\n",
      "Most similar words to: c \n",
      " [('c++', 0.7438018321990967), ('pointers', 0.6658585071563721), ('boost', 0.6583142876625061), ('buffer', 0.652622640132904), ('printf', 0.6419030427932739), ('malloc', 0.6326224207878113), ('stl', 0.6298226118087769), ('gcc', 0.6296911835670471), ('standard', 0.6215011477470398), ('stdstring', 0.6205230951309204)] \n",
      "\n",
      "\n",
      "Most similar words to: c++ \n",
      " [('pointers', 0.7485740184783936), ('c', 0.7438018321990967), ('boost', 0.7069777846336365), ('gcc', 0.704820454120636), ('malloc', 0.7042771577835083), ('stl', 0.7031874656677246), ('macro', 0.6916307806968689), ('unmanaged', 0.6779928207397461), ('printf', 0.6776496767997742), ('structs', 0.675813615322113)] \n",
      "\n",
      "\n",
      "Most similar words to: cuda \n",
      " [('optimization', 0.9562216401100159), ('fstream', 0.9371057152748108), ('eat', 0.9365462064743042), ('questions', 0.9358989000320435), ('abnormally', 0.9354915618896484), ('arm', 0.9347497224807739), ('loadlibrary', 0.9347289204597473), ('fpic', 0.9346720576286316), ('potential', 0.9334027767181396), ('msvc', 0.9333370327949524)] \n",
      "\n",
      "\n",
      "Most similar words to: gpu \n",
      " [('pro', 0.9695045948028564), ('minimal', 0.9692796468734741), ('rubygems', 0.9684691429138184), ('beginners', 0.9681559801101685), ('complains', 0.9681438207626343), ('36', 0.9678215980529785), ('461', 0.9663509130477905), ('mercurial', 0.9654421806335449), ('platforms', 0.9653660655021667), ('higher', 0.964167058467865)] \n",
      "\n",
      "\n",
      "Most similar words to: cpu \n",
      " [('high', 0.9243365526199341), ('jvm', 0.9148995280265808), ('much', 0.9072808623313904), ('slower', 0.9062235355377197), ('windows7', 0.9041781425476074), ('107', 0.9013205170631409), ('32bit', 0.900380551815033), ('intel', 0.8982457518577576), ('lion', 0.8965007066726685), ('corruption', 0.895294725894928)] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['python', 'java', 'javascript', 'c', 'c++', 'cuda', 'gpu', 'cpu']\n",
    "\n",
    "for word in words:\n",
    "    print(f'Most similar words to: {word}','\\n', model.wv.most_similar(word), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06298765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.index_to_key), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.wv.index_to_key]\n",
    "        \n",
    "    word_vectors = np.array([model.wv[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6b39bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_pca_scatterplot(model, ['python', 'java', 'javascript', 'c', 'c++', 'cuda', 'gpu', 'cpu'])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
